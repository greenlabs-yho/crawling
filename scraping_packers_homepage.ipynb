{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yoonhae/.pyenv/versions/3.8.12/envs/crawling/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.vertexai.VertexAI` was deprecated in langchain-community 0.0.12 and will be removed in 0.2.0. An updated version of the class exists in the langchain-google-vertexai package and should be used instead. To use it run `pip install -U langchain-google-vertexai` and import as `from langchain_google_vertexai import VertexAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['homepage', 'extract_url', 'company_name', 'mail', 'phone', 'address', 'fax', 'products', 'error']\n"
     ]
    }
   ],
   "source": [
    "import scraping_packers_homepage.env as env\n",
    "llm = env.llm\n",
    "target_column_list = env.TARGET_COLUMN_LIST\n",
    "print(target_column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** [main page] 조회 - https://www.classicpaints.com.fj\n",
      "  ** [llm] contact 정보 & products 추출\n",
      "    ** split text - chunk : 2\n",
      "    {\n",
      "        \"contact\": {\n",
      "            \"email\": null,\n",
      "            \"phone\": null,\n",
      "            \"address\": null\n",
      "        },\n",
      "        \"products\": [\n",
      "            \"APCO\",\n",
      "            \"CarPro\",\n",
      "            \"Cromax\",\n",
      "            \"Dulux\",\n",
      "            \"Easicoat\",\n",
      "            \"Meguiars\",\n",
      "            \"PPG\",\n",
      "            \"Syrox\",\n",
      "            \"3M\"\n",
      "        ]\n",
      "    }\n",
      "    {\n",
      "        \"contact\": {\n",
      "            \"email\": null,\n",
      "            \"phone\": null,\n",
      "            \"address\": null\n",
      "        },\n",
      "        \"products\": [\n",
      "            \"APCO\",\n",
      "            \"MEGUIAR'S\",\n",
      "            \"CROMAX\",\n",
      "            \"CARPRO\",\n",
      "            \"DULUX\",\n",
      "            \"3M\",\n",
      "            \"SYROX\"\n",
      "        ]\n",
      "    }\n",
      "  ** a tag 추출\n",
      "  ** [llm] 회사 정보와 관련있는 a tag 선별\n",
      "    ['https://www.classicpaints.com.fj/products/']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scraping_packers_homepage.web_scraper as web_scraper\n",
    "import scraping_packers_homepage.llm_models.firm_info_extractor as firm_info_extractor\n",
    "import scraping_packers_homepage.llm_models.a_tag_selector as a_tag_selector\n",
    "import importlib\n",
    "# 모듈이 수정된 후에, 다음 코드를 실행하여 모듈을 다시 로드합니다.\n",
    "importlib.reload(web_scraper)\n",
    "\n",
    "\n",
    "url_list = [\n",
    "    #'https://www.naturzgroup.com/',  # 60\n",
    "    #'https://kayalfoods.in/',  # 4556\n",
    "    #\"http://www.great-lines.com/about-us/\"\n",
    "    #\"http://bonanzamegahoil.com\"\n",
    "    #\"https://www.classicpaints.com.fj\"\n",
    "    \"https://www.exportersindia.com/kube-exim/\"\n",
    "    #\"https://www.escurette.fr/\"\n",
    "    #'https://www.torresyribelles.com/home2'\n",
    "]\n",
    "\n",
    "#result_df = pd.DataFrame(columns=target_column_list, dtype=str)\n",
    "for url in url_list:\n",
    "    sub_df = web_scraper.run(url, llm, target_column_list, required_column_list=[\"mail\", \"phone\", \"products\"])\n",
    "    #result_df = result_df.append(sub_df, ignore_index=True)\n",
    "\n",
    "#len(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_list(series):\n",
    "    # NaN 값을 제거하고, 중복을 제거한 후 리스트로 변환\n",
    "    return series.dropna().drop_duplicates().tolist()\n",
    "\n",
    "\n",
    "# 'homepage'를 기준으로 그룹화하고 각 컬럼에 unique_list 함수 적용\n",
    "grouped_df = result_df.groupby('homepage').agg(unique_list).reset_index()\n",
    "\n",
    "def convert_to_string(value):\n",
    "    # 값이 리스트인 경우\n",
    "    if isinstance(value, list):\n",
    "        # 빈 리스트인 경우 None 반환main\n",
    "        if not value:\n",
    "            return ''\n",
    "        # 리스트의 요소들을 문자열로 결합\n",
    "        return \";\\n\".join(str(item) for item in value if pd.notna(item) and isinstance(item, str) and item)\n",
    "    # 리스트가 아닌 경우, 값을 그대로 반환\n",
    "    return value\n",
    "\n",
    "# 'homepage' 컬럼을 제외한 모든 컬럼에 대해 함수 적용\n",
    "for col in grouped_df.columns:\n",
    "    if col != 'homepage':\n",
    "        grouped_df[col] = grouped_df[col].apply(convert_to_string)\n",
    "\n",
    "print(grouped_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grouped_df[\"products\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "grouped_df.to_csv('./output/check.csv', quoting=csv.QUOTE_ALL, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 작업 방법 노트!!!!\n",
    "\n",
    "1. 컬럼은 \"이메일 작업 대상\" 이 있는곳까지만 있는지 체크.\n",
    "2. 작업대상 컬럼들은 진짜 다음 스텝에서 작업해야하는 경우에만 체크해야한다.\n",
    "3. 홈페이지 크롤링 통해서 추가 데이터가 수집된 경우 작업대상을 그때그때 마킹한다.\n",
    "4. 작업 마무리 되면 gcs/.../raw  폴더에 drop\n",
    "\n",
    "## 고민 포인트\n",
    "> => homepage 크롤링의 작업 소요 시간이 길다. -> 분산처리 필요.\n",
    ">    => 분산처리를 어떻게 구성할 것인가?\n",
    "> => 한번 작업한 homepage 에 대한 상세 내역을 따로 저장해둘것인가?\n",
    ">    => 작업한 이력이 확인되면 저장된 값을 그대로 갖다 쓰는 방향으로?\n",
    "\n",
    "## 고민 끝나고 작업하기로! 일단 ai 챗봇하러 간당... 20240130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "new_df = pd.read_csv('/Users/yoonhae/Downloads/(그레인스캐너-데이터팀) 패커 크롤링 - paris_show_재작업.csv')\n",
    "\n",
    "job_config = {\n",
    "    \"homepage_column\": '홈페이지 주소',\n",
    "    \"target_column_dict\": {\n",
    "        \"address\": {\n",
    "            \"read_col_list\": ['국가(영어로)', '주소 raw'],\n",
    "            \"write_col\": '주소 raw',\n",
    "        },\n",
    "        \"products\": {\n",
    "            \"read_col_list\": ['SKU raw', '취급 SKU1'],\n",
    "            \"write_col\": 'SKU raw',\n",
    "        },\n",
    "        \"phone\": {\n",
    "            \"read_col_list\": ['전화번호', '기존 전화번호'],\n",
    "            \"write_col\": '기존 전화번호',\n",
    "        },\n",
    "        \"mail\": {\n",
    "            \"read_col_list\": ['대표 이메일'],\n",
    "            \"write_col\": '대표 이메일',\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def unique_list(series):\n",
    "    # NaN 값을 제거하고, 중복을 제거한 후 리스트로 변환\n",
    "    return series.dropna().drop_duplicates().tolist()\n",
    "\n",
    "\n",
    "def convert_to_string(value):\n",
    "    # 값이 리스트인 경우\n",
    "    if isinstance(value, list):\n",
    "        # 빈 리스트인 경우 None 반환\n",
    "        if not value:\n",
    "            return ''\n",
    "        # 리스트의 요소들을 문자열로 결합\n",
    "        return \";\\n\".join(str(item) for item in value if pd.notna(item) and isinstance(item, str) and item)\n",
    "    # 리스트가 아닌 경우, 값을 그대로 반환\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'scraping_packers_homepage.web_scraper' from '/Users/yoonhae/test/crawling/scraping_packers_homepage/web_scraper.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import scraping_packers_homepage.web_scraper as web_scraper\n",
    "import scraping_packers_homepage.llm_models.a_tag_selector as a_tag_selector\n",
    "import scraping_packers_homepage.llm_models.firm_info_extractor as firm_info_extractor\n",
    "import importlib\n",
    "# 모듈이 수정된 후에, 다음 코드를 실행하여 모듈을 다시 로드합니다.\n",
    "\n",
    "#importlib.reload(a_tag_selector)\n",
    "importlib.reload(web_scraper)\n",
    "#importlib.reload(firm_info_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['패커명*'][347]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scraping_packers_homepage.web_scraper as web_scraper\n",
    "homepage_col = job_config['homepage_column']\n",
    "target_column_dict = job_config['target_column_dict']\n",
    "\n",
    "for index, row in new_df.iterrows():\n",
    "    # if index < 348:\n",
    "    #     continue\n",
    "    # 홈페이지 주소가 있는 경우\n",
    "    if not pd.isna(row[homepage_col]):\n",
    "        required_column_list = []\n",
    "        # 수집할 컬럼리스트 추출\n",
    "        for check_col, job_info in target_column_dict.items():\n",
    "            if any(not pd.isna(row[col]) for col in job_info['read_col_list'] if col in row):\n",
    "                # 이미 있어서 구지 수집할 필요없음.\n",
    "                pass\n",
    "            else:\n",
    "                required_column_list.append(check_col)\n",
    "        # print(row.tolist())\n",
    "        # print(row[homepage_col], required_column_list)\n",
    "\n",
    "        # 필요한 데이터가 있는 경우\n",
    "        if required_column_list and 'http://www.ladrome.fr/' != row[homepage_col]:\n",
    "            # 스크래핑 작업 진행.\n",
    "            sub_df = web_scraper.run(row[homepage_col], llm, target_column_list, required_column_list=required_column_list)\n",
    "            # 사이트별 하나로 그루핑\n",
    "            sub_df = sub_df.groupby('homepage').agg(unique_list).reset_index()\n",
    "\n",
    "            for col in required_column_list:\n",
    "                # 값이 있는 경우\n",
    "                if sub_df[col].notna().any() and not sub_df[col].empty:\n",
    "                    # 그루핑 값을 string 기반으로 변경\n",
    "                    sub_df[col] = sub_df[col].apply(convert_to_string)\n",
    "                    # 저장\n",
    "                    new_df.at[index, target_column_dict[col]['write_col']] = sub_df[col].iloc[0]\n",
    "\n",
    "    if (index % 10) == 9:\n",
    "        new_df.to_csv('./output/salon_agriculture_20240206_1222.csv', index=False, quoting=1)\n",
    "\n",
    "new_df.to_csv('./output/salon_agriculture_20240206_1222.csv', index=False, quoting=1)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "# 실행하고자 하는 함수\n",
    "def my_function():\n",
    "    # 여기에 로직 구현\n",
    "    time.sleep(30)\n",
    "    pass\n",
    "\n",
    "# 타임아웃 시간 (초 단위)\n",
    "timeout_seconds = 5\n",
    "\n",
    "# ThreadPoolExecutor를 사용하여 별도의 스레드에서 함수 실행\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n",
    "    # Future 객체를 사용하여 함수 실행\n",
    "    future = executor.submit(my_function)\n",
    "    try:\n",
    "        # 지정된 타임아웃 시간 내에 함수의 실행 결과를 기다림\n",
    "        result = future.result(timeout=timeout_seconds)\n",
    "    except concurrent.futures.TimeoutError:\n",
    "        print(\"타임아웃: 함수 실행 시간이 초과되었습니다.\")\n",
    "        # 필요한 예외 처리 또는 로직을 여기에 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv('output/크롤링 데이터 정리_cotton - GMP+ FRA,FSA 1차정리_from_homepage_20240129.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"new_contry\" in new_df.columns:\n",
    "    new_df[\"국가(영어로)\"] = new_df[\"new_contry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "filtered_df = new_df.iloc[:, :new_df.columns.get_loc('이메일 작업 대상') + 1]\n",
    "filtered_df.to_csv(\"./output/크롤링 데이터 정리_cotton - GMP+ FRA,FSA 1차정리_from_homepage_20240130.csv\", quoting=csv.QUOTE_ALL, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np    \n",
    "\n",
    "new_df[\"주소 작업대상\"] = ''\n",
    "new_df['SKU 작업대상'] = ''\n",
    "new_df['이메일 작업 대상'] = ''\n",
    "\n",
    "new_df['주소 작업대상'] = np.where((new_df['주소 raw'].notnull() & new_df['주소 raw'].notna() & (new_df['주소 raw'] != '')) &\n",
    "                   (new_df['국가(영어로)'].isnull() | new_df['국가(영어로)'].isna() | (new_df['국가(영어로)'] == '')),\n",
    "                   '작업대상', '')\n",
    "\n",
    "new_df['SKU 작업대상'] = np.where((new_df['SKU raw'].notnull() & new_df['SKU raw'].notna() & (new_df['SKU raw'] != '')) &\n",
    "                   (new_df['취급 SKU1'].isnull() | new_df['취급 SKU1'].isna() | (new_df['취급 SKU1'] == '')),\n",
    "                   '작업대상', '')\n",
    "if (new_df['참조 이메일(여러개 입력시 콤마(,)로 구분)'].notna() & (new_df['참조 이메일(여러개 입력시 콤마(,)로 구분)'] != '')).any() and \\\n",
    "   ((new_df['대표 이메일'].str.count('@') > 1) | \n",
    "    (new_df['대표 이메일'].str.contains('[ ,;]'))).any():\n",
    "    new_df['이메일 작업 대상'] = np.where((new_df['대표 이메일'].notnull() & new_df['대표 이메일'].notna() & (new_df['대표 이메일'] != '')),\n",
    "                                '작업대상', '')\n",
    "    \n",
    "filtered_df = new_df.iloc[:, :new_df.columns.get_loc('이메일 작업 대상') + 1]\n",
    "filtered_df.to_csv(\"./output/크롤링 데이터 정리_cotton - GMP+ FRA,FSA 1차정리_from_homepage_20240130.csv\", quoting=csv.QUOTE_ALL, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.adapters import HTTPAdapter, Retry\n",
    "import requests\n",
    "\n",
    "session = requests.Session()\n",
    "retries = Retry(total=2, backoff_factor=1, status_forcelist=[500, 502, 503, 504])\n",
    "session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "session.timeout = 5\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}\n",
    "response = session.get(\"http://www.ladrome.fr/\", timeout=5, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
