{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greenlabs 한글 きゆん esto es espanol bu turkce1 / green labs\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "replacement_mapping = {\n",
    "    'À': 'A', 'Á': 'A', 'Â': 'A', 'Ã': 'A', 'Ä': 'A', 'Å': 'A', 'Ā': 'A', \n",
    "    'Æ': 'AE', \n",
    "    'Ç': 'C', 'Č': 'C', \n",
    "    'Ð': 'D', \n",
    "    'Đ': 'Dj', \n",
    "    'È': 'E', 'É': 'E', 'Ê': 'E', 'Ë': 'E', 'Ē': 'E', \n",
    "    'Ğ': 'G', \n",
    "    'I': 'I', 'Ì': 'I', 'Í': 'I', 'Î': 'I', 'Ï': 'I', 'Ī': 'I', 'İ': 'I', \n",
    "    'Ł': 'L', \n",
    "    'Ñ': 'N', 'Ń': 'N', \n",
    "    'Ò': 'O', 'Ó': 'O', 'Ô': 'O', 'Õ': 'O', 'Ö': 'O', 'Ø': 'O', 'Ō': 'O', \n",
    "    'Ś': 'S', 'Ş': 'S', 'Š': 'S', \n",
    "    'Þ': 'TH', \n",
    "    'Ù': 'U', 'Ú': 'U', 'Û': 'U', 'Ü': 'U', 'Ū': 'U', \n",
    "    'Ý': 'Y', \n",
    "    'Ź': 'Z', 'Ż': 'Z', 'Ž': 'Z', \n",
    "    'à': 'a', 'á': 'a', 'â': 'a', 'ã': 'a', 'ä': 'a', 'å': 'a', 'ā': 'a', \n",
    "    'æ': 'ae', \n",
    "    'ç': 'c', 'č': 'c', \n",
    "    'ð': 'd', \n",
    "    'đ': 'dj', \n",
    "    'è': 'e', 'é': 'e', 'ê': 'e', 'ë': 'e', 'ē': 'e', \n",
    "    'ğ': 'g', \n",
    "    'i': 'i', 'ì': 'i', 'í': 'i', 'î': 'i', 'ï': 'i', 'ī': 'i', 'ı': 'i', \n",
    "    'ł': 'l', \n",
    "    'ñ': 'n', 'ń': 'n', \n",
    "    'ò': 'o', 'ó': 'o', 'ô': 'o', 'õ': 'o', 'ö': 'o', 'ø': 'o', 'ō': 'o', \n",
    "    'ś': 's', 'ş': 's', 'š': 's', \n",
    "    'ß': 'ss', \n",
    "    'þ': 'th', \n",
    "    'ù': 'u', 'ú': 'u', 'û': 'u', 'ü': 'u', 'ū': 'u', \n",
    "    'ý': 'y', 'ÿ': 'y', \n",
    "    'ź': 'z', 'ż': 'z', 'ž': 'z'}\n",
    "\n",
    "def replace_special_characters(text):\n",
    "    \"\"\"\n",
    "    주어진 텍스트에서 특수 문자를 영문으로 대체한다.\n",
    "    \"\"\"\n",
    "    for original_char, replacement_char in replacement_mapping.items():\n",
    "        text = text.replace(original_char, replacement_char)\n",
    "    return text\n",
    "\n",
    "def cleansing_name(name: str, remove_space=False) -> str:\n",
    "    name = replace_special_characters(name) # 영문으로 치환가능한 문자 치환\n",
    "    name = name.lower()     # 소문자\n",
    "    name = name.replace('&', 'and') # & 처리\n",
    "\n",
    "    suffixes = [\n",
    "        \"ltd\", \"co\", \"co ltd\", \"coltd\", \"company limited\", \"companylimited\",\n",
    "        \"private limited\", \"privatelimited\", \"limited\", \"llc\", \"sac\", \"sa\", \"inc\",\n",
    "        \"co-op\", \"inc\", \"pty ltd\", \"ptyltd\", \"gmbh\", \"eirl\", \"sarl\", \"ltd sti\", \"ltdsti\"\n",
    "    ]\n",
    "\n",
    "    # 정규식 패턴 생성: 접미사들을 '|'로 연결하여 선택적으로 매치하도록 함\n",
    "    # 각 접미사는 단어 경계(\\b)로 둘러싸여 있으며, 접미사 앞에 공백이 있을 수 있음\n",
    "    pattern = r'\\b(?:' + '|'.join(re.escape(suffix) for suffix in suffixes) + r')\\b'\n",
    "    name = re.sub(pattern, '', name, flags=re.IGNORECASE)\n",
    "\n",
    "    # 숫자 문자 외 모두 제거(특수문자, 공백)\n",
    "    if remove_space:\n",
    "        pattern = r'[^\\w]'\n",
    "    else:\n",
    "        pattern = r'[^\\w\\s]'\n",
    "    name = re.sub(pattern, '', name, flags=re.UNICODE).strip()\n",
    "\n",
    "    return name\n",
    "\n",
    "target_name = 'Greenlabs 한글 きゆん。 Esto es español, Bu Türkçe.1'\n",
    "search_name = 'Green Labs Llc'\n",
    "print(cleansing_name(target_name), \"/\", cleansing_name(search_name))\n",
    "\n",
    "from scraping_packers_homepage.tools.text_similarity import measure_text_similarity\n",
    "print(measure_text_similarity(cleansing_name(target_name), cleansing_name(search_name)))\n",
    "\n",
    "def find_name_from_title(title, cleansed_comp_name):\n",
    "    word_list = re.split(',|\\||-|/', title) # , | - / 기준으로 split\n",
    "    score_dict = {}\n",
    "    for word in word_list:\n",
    "        word = word.strip()\n",
    "        cleansed_word = cleansing_name(word)\n",
    "        score_dict[measure_text_similarity(cleansing_name(cleansed_comp_name), cleansing_name(cleansed_word))] = word\n",
    "    \n",
    "    max_key = max(score_dict, key=lambda k: k)\n",
    "    value = score_dict[max_key] if max_key is not None else None\n",
    "    return max_key, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, json\n",
    "from google.cloud import bigquery\n",
    "import scraping_packers_homepage.tools.bing_searcher as bing_searcher\n",
    "\n",
    "query = \"\"\"\n",
    "select uuid, data_type, raw_data, processed_data\n",
    "from `greenlabs-data-farmmorning.content_analysis.gs_crawling_packer_info_processed`\n",
    "where source_id = 'test_verity_20240222'\n",
    "and (process_ts between '2024-02-23 03:51:05' and '2024-02-23 03:53:05')\n",
    "\"\"\"\n",
    "\n",
    "bigquery_client = bigquery.Client(project=\"grainscanner\")\n",
    "df = bigquery_client.query(query).to_dataframe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* site:panjiva.com Montana Milling Inc [panjiva.com/Montana West Inc./montana west]\n",
      "******* site:www.importgenius.co.kr Montana Milling Inc [www.importgenius.co.kr/Montana West Inc./montana west]\n",
      "******* site:www.exportgenius.in Montana Milling Inc [www.exportgenius.in/Montana West Inc./montana west]\n",
      "******* site:www.seair.co.in Montana Milling Inc [www.seair.co.in/Montana West Inc./montana west]\n",
      "******* site:importkey.com Montana Milling Inc [importkey.com/Montana West Inc./montana west]\n",
      "******* site:panjiva.com HI Plains Agronomy LLC [panjiva.com/HI Plains Silage Llc/hi plains silage]\n",
      "******* site:www.importgenius.co.kr HI Plains Agronomy LLC [www.importgenius.co.kr/HI Plains Silage Llc/hi plains silage]\n",
      "******* site:www.exportgenius.in HI Plains Agronomy LLC [www.exportgenius.in/HI Plains Silage Llc/hi plains silage]\n",
      "******* site:www.seair.co.in HI Plains Agronomy LLC [www.seair.co.in/HI Plains Silage Llc/hi plains silage]\n",
      "******* site:importkey.com HI Plains Agronomy LLC [importkey.com/HI Plains Silage Llc/hi plains silage]\n",
      "******* site:panjiva.com Puris Grains [panjiva.com/Puris Grains/puris grains]\n",
      "******* site:www.importgenius.co.kr Puris Grains [www.importgenius.co.kr/Puris Grains/puris grains]\n",
      "******* site:www.exportgenius.in Puris Grains [www.exportgenius.in/Puris Grains/puris grains]\n",
      "******* site:www.seair.co.in Puris Grains [www.seair.co.in/Puris Grains/puris grains]\n",
      "******* site:importkey.com Puris Grains [importkey.com/Puris Grains/puris grains]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5405.03it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "source_id = \"test_verity_20240222\"\n",
    "result_list = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    search_keyword = row[\"raw_data\"]\n",
    "    result = row[\"processed_data\"]\n",
    "\n",
    "    domain = search_keyword.split(' ')[0][len(\"site:\"):]\n",
    "    comp_name = search_keyword[len(search_keyword.split(' ')[0]):].strip()\n",
    "\n",
    "    cleansed_comp_name = cleansing_name(comp_name)\n",
    "    \n",
    "    if result and result.startswith('{'):\n",
    "        j_result = json.loads(row[\"processed_data\"])\n",
    "        if j_result.get(\"webPages\") and j_result[\"webPages\"].get(\"value\"):\n",
    "            success_list = []\n",
    "            fail_list = []\n",
    "            for item in j_result[\"webPages\"][\"value\"]:\n",
    "                if domain not in item[\"url\"]:\n",
    "                    print (\"  ** skip(domain not matched) -\", item)\n",
    "                    continue\n",
    "\n",
    "                score, item_name = find_name_from_title(item[\"name\"], cleansed_comp_name)\n",
    "                cleansed_item_name = cleansing_name(item_name)\n",
    "                score_item = {\n",
    "                    \"score\": 100 if cleansed_comp_name.replace(' ', '') == cleansed_item_name.replace(' ', '') else score,\n",
    "                    \"title\": item[\"name\"],\n",
    "                    \"domain\": domain,\n",
    "                    \"url\": item[\"url\"]\n",
    "                }\n",
    "                if score_item[\"score\"] == 100:    \n",
    "                    success_list.append(score_item)\n",
    "                else:\n",
    "                    fail_list.append(score_item)\n",
    "            if not success_list:  # 성공한게 없으면 실패 리스트중에 80점 이상인 것이 있는지 확인.\n",
    "                max_fail = max(fail_list, key=lambda r: r[\"score\"])\n",
    "                if max_fail[\"score\"] >= 70:  # 부분 일치\n",
    "                    success_list.append(max_fail)\n",
    "\n",
    "            if success_list:\n",
    "                result_list.append({\n",
    "                    \"source_id\": source_id,\n",
    "                    \"uuid\": row[\"uuid\"],\n",
    "                    \"data_type\": row[\"data_type\"],\n",
    "                    \"job_type\": \"detect_search_result\",\n",
    "                    \"job_detail\": \"bing\",\n",
    "                    \"raw_data\": row[\"raw_data\"],\n",
    "                    \"processed_data\": json.dumps(success_list, ensure_ascii=False)\n",
    "                })\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame(result_list)\n",
    "result_df.to_gbq(\"content_analysis.gs_crawling_packer_info_processed\",\n",
    "          \"greenlabs-data-farmmorning\",\n",
    "          if_exists=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source_id': 'test_verity_20240222',\n",
       "  'data_type': 'validation_import',\n",
       "  'job_type': 'detect_search_result',\n",
       "  'job_detail': 'bing',\n",
       "  'raw_data': 'site:panjiva.com Montana Milling Inc',\n",
       "  'processed_data': '[{\"score\": 100, \"title\": \"Montana West Inc., 2606 Brenner Drive, Dallas, TX 75220, USA | Buyer Report — Panjiva\", \"url\": \"https://cn.panjiva.com/Montana-West-Inc/61340517\"}, {\"score\": 100, \"title\": \"Montana West Inc. - Panjiva\", \"url\": \"https://panjiva.com/Montana-West-Inc/5402821\"}]'},\n",
       " {'source_id': 'test_verity_20240222',\n",
       "  'data_type': 'validation_import',\n",
       "  'job_type': 'detect_search_result',\n",
       "  'job_detail': 'bing',\n",
       "  'raw_data': 'site:panjiva.com HI Plains Agronomy LLC',\n",
       "  'processed_data': '[{\"score\": 94, \"title\": \"High Plains Silage Llc - Panjiva\", \"url\": \"https://panjiva.com/High-Plains-Silage-Llc/20918244\"}]'}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from google.cloud import bigquery\n",
    "import scraping_packers_homepage.tools.bing_searcher as bing_searcher\n",
    "\n",
    "query = \"\"\"\n",
    "select source_id\n",
    "    , uuid\n",
    "    , packer_name\n",
    "    , country\n",
    "    , JSON_STRIP_NULLS(json_Array(handled_sku1, handled_sku2, handled_sku3, handled_sku4, handled_sku5, handled_sku6)) as sku_list\n",
    "from `greenlabs-data-farmmorning.content_analysis.gs_crawling_packer_info_raw`\n",
    "where source_id = 'test_verity_20240222'\n",
    "\"\"\"\n",
    "\n",
    "bigquery_client = bigquery.Client(project=\"grainscanner\")\n",
    "query_job = bigquery_client.query(query)\n",
    "query_job.result()\n",
    "\n",
    "site_list = ['panjiva.com',\n",
    "             'www.importgenius.co.kr',\n",
    "             'www.exportgenius.in',\n",
    "             'www.seair.co.in',\n",
    "             'importkey.com']\n",
    "\n",
    "import json\n",
    "MAX_INSERT_PACKER = 2\n",
    "\n",
    "result_list = []\n",
    "for i, row in enumerate(query_job):\n",
    "    \n",
    "    packer_name = row.packer_name\n",
    "    for site in site_list:\n",
    "        query = f\"site:{site} {packer_name}\"\n",
    "        try:\n",
    "            params = { 'count': 50, 'responseFilter':'Webpages' }\n",
    "            result = json.dumps(bing_searcher.search(query, '2d20c255950948d6b0ffa9685720117d', kwargs=params), ensure_ascii=False)\n",
    "        except Exception as e:\n",
    "            result = str(e)\n",
    "        \n",
    "        result_list.append({\n",
    "            \"source_id\": row.source_id,\n",
    "            \"uuid\": row.uuid,\n",
    "            \"data_type\": \"validation_import\",\n",
    "            \"job_type\": \"search\",\n",
    "            \"job_detail\": \"bing\",\n",
    "            \"raw_data\": query,\n",
    "            \"processed_data\": result\n",
    "        })\n",
    "        time.sleep(0.1)\n",
    "        break\n",
    "    \n",
    "    if (i%MAX_INSERT_PACKER) == (MAX_INSERT_PACKER-1):\n",
    "        print(f' ** flush - {len(result_list)} rows')\n",
    "        bigquery_client.insert_rows_json(\"greenlabs-data-farmmorning.content_analysis.gs_crawling_packer_info_processed\", result_list)\n",
    "        result_list = []\n",
    "    break\n",
    "\n",
    "if result_list:\n",
    "    bigquery_client.insert_rows_json(\"greenlabs-data-farmmorning.content_analysis.gs_crawling_packer_info_processed\", result_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'United States'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT p.id, p.name, array_agg(distinct cc.name ignore nulls) as country, array_agg(distinct sku.name_en ignore nulls) as sku\n",
    "FROM `greenlabs-data-farmmorning.stream_ods_grainscanner.packer` as p\n",
    "inner join `greenlabs-data-farmmorning.stream_ods_grainscanner.packer_country` as pc\n",
    "  on p.id = pc.packer_id\n",
    "inner join `greenlabs-data-farmmorning.stream_ods_grainscanner.common_country` as cc\n",
    "  on pc.country_id = cc.id\n",
    "left outer join `greenlabs-data-farmmorning.stream_ods_grainscanner.packer_sku` as ps\n",
    "  on p.id = ps.packer_id\n",
    "left outer join `greenlabs-data-farmmorning.stream_ods_grainscanner.sku` as sku\n",
    "  on ps.sku_id = sku.id\n",
    "WHERE p.is_deleted = 0 \n",
    "  and p.contact_stage = 'PCS01' \n",
    "  and p.verification_stage = 'PVS01'\n",
    "group by 1, 2\n",
    "order by 1, 2\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
